{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation for Amazon Fashion Dataset\n",
    "\n",
    "### Overview\n",
    "This notebook documents the process of downloading, cleaning, analyzing, and preparing the Amazon Fashion dataset for further use, such as building a vector database. The dataset contains product information, including titles, ratings, features, descriptions, prices, and images. The goal is to optimize the dataset by removing unnecessary columns, handling missing data, and reducing its size to meet GitHub's 100MB file size limit.\n",
    "\n",
    "### Steps and Workflow\n",
    "- **Initial Setup**:\n",
    "    - Imported necessary libraries (`pandas`, `matplotlib`, etc.).\n",
    "    - Configured pandas options for better display of data.\n",
    "- **Data Downloading and Loading**:\n",
    "    - Downloaded the dataset from a specified URL and saved it locally as a compressed JSONL file.\n",
    "    - Loaded the dataset into a pandas DataFrame.\n",
    "- **Exploratory Data Analysis (EDA)**:\n",
    "    - Inspected the structure and content of the dataset using `.head()`, `.columns`, and `.iloc`.\n",
    "    - Analyzed the distribution of key columns like `main_category`, `average_rating`, and `rating_number`.\n",
    "- **Data Cleaning**:\n",
    "    - Dropped irrelevant or empty columns such as `bought_together`, `videos`, `main_category`, and `categories`.\n",
    "    - Extracted and retained only the `MAIN` `HIGH_RES` image URLs.\n",
    "    - Concatenated multi-part descriptions into a single string.\n",
    "    - Removed unnecessary prefixes from image URLs to reduce file size.\n",
    "- **Data Optimization**:\n",
    "    - Converted data types (e.g., `float32`, `int32`, `category`) to save memory.\n",
    "    - Randomly dropped 5% of the data to reduce the dataset size.\n",
    "- **Data Export**:\n",
    "    - Saved the cleaned and optimized dataset as a compressed CSV file.\n",
    "\n",
    "### Main Findings\n",
    "- The dataset contained several columns with missing or irrelevant data:\n",
    "    - `price` was missing for the majority of products.\n",
    "    - `bought_together` and `categories` were fully null and dropped.\n",
    "    - `main_category` had no variation and was removed.\n",
    "- The `features` and `details` columns provided valuable information about the products.\n",
    "- All non-null image URLs followed a consistent format, allowing for efficient processing.\n",
    "- Multi-part descriptions were concatenated to improve usability.\n",
    "\n",
    "### Results\n",
    "- The cleaned dataset contains 784,803 rows and 9 columns.\n",
    "- The dataset size was reduced to below 100MB by:\n",
    "    - Dropping unnecessary columns.\n",
    "    - Optimizing data types.\n",
    "    - Removing 5% of the data randomly.\n",
    "- The final dataset was saved as a compressed CSV file (`products.csv`) for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will help us later when handling columns of dtype list\n",
    "def flatten_and_count(df, column):\n",
    "    \"\"\"\n",
    "    Flatten a column of lists and count occurrences of each unique item.\n",
    "    \"\"\"\n",
    "    # Flatten all category lists (filtering out non-lists or NaNs)\n",
    "    flattened = [\n",
    "        category for row in df[column] if isinstance(row, list) for category in row\n",
    "    ]\n",
    "\n",
    "    # Count occurrences\n",
    "    category_counts = Counter(flattened)\n",
    "\n",
    "    # Convert to DataFrame for better readability or export\n",
    "    category_df = pd.DataFrame.from_dict(\n",
    "        category_counts, orient=\"index\", columns=[\"count\"]\n",
    "    ).sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    return category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists at ./raw/meta_Amazon_Fashion.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset and the local path where it will be saved\n",
    "url = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Amazon_Fashion.jsonl.gz\"\n",
    "output_path = \"./data/meta_Amazon_Fashion.jsonl.gz\"\n",
    "\n",
    "# Check if the file already exists at the specified path\n",
    "file_exists = os.path.exists(output_path)\n",
    "\n",
    "# Print a message based on whether the file exists or not\n",
    "if file_exists:\n",
    "    print(f\"The file already exists at {output_path}\")\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    print(f\"Created directories for the path {output_path}\")\n",
    "    print(f\"No file found at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists at ./raw/meta_Amazon_Fashion.jsonl.gz. Skipped download.\n"
     ]
    }
   ],
   "source": [
    "# Check if the file already exists\n",
    "if not file_exists:\n",
    "    # If the file does not exist, download it from the specified URL\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        # Write the file in chunks to avoid memory issues with large files\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"File downloaded and saved to {output_path}\")\n",
    "    else:\n",
    "        # Print an error message if the download fails\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    # If the file already exists, print a message\n",
    "    print(f\"The file already exists at {output_path}. Skipped download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloaded file into a pandas DataFrame\n",
    "df = pd.read_json(output_path, lines=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 826108 entries, 0 to 826107\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   main_category    826108 non-null  object \n",
      " 1   title            826108 non-null  object \n",
      " 2   average_rating   826108 non-null  float64\n",
      " 3   rating_number    826108 non-null  int64  \n",
      " 4   features         826108 non-null  object \n",
      " 5   description      826108 non-null  object \n",
      " 6   price            50249 non-null   float64\n",
      " 7   images           826108 non-null  object \n",
      " 8   videos           826108 non-null  object \n",
      " 9   store            799270 non-null  object \n",
      " 10  categories       826108 non-null  object \n",
      " 11  details          826108 non-null  object \n",
      " 12  parent_asin      826108 non-null  object \n",
      " 13  bought_together  0 non-null       float64\n",
      "dtypes: float64(3), int64(1), object(10)\n",
      "memory usage: 88.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns of the DataFrame to understand its structure\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into a random row\n",
    "df.iloc[3].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont need all images for our prototype so let's pick up 1 image only - which is the `MAIN` `HIGH_RES` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"main_hi_res_image\"] = df[\"images\"].apply(\n",
    "    lambda images: (\n",
    "        next(\n",
    "            (\n",
    "                img.get(\"hi_res\")\n",
    "                for img in images\n",
    "                if img.get(\"variant\", \"\").lower() == \"main\"\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if isinstance(images, list)\n",
    "        else None\n",
    "    )\n",
    ")\n",
    "df = df.drop(\"images\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a dataframe that summarizes the products\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Column Name\": df.columns,\n",
    "        \"Data Type\": df.dtypes.values,\n",
    "        \"Non-Null Count\": df.notnull().sum().values,\n",
    "        \"Null Count\": df.isnull().sum().values,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that `price` is missing for majority of products.\n",
    "- `bought_together` is fully null and can be dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"bought_together\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also remove videos as it won't be needed in our prototype\n",
    "df = df.drop(\"videos\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check main category distribution\n",
    "df[\"main_category\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no variation so we can safely drop this columns\n",
    "df = df.drop(\"main_category\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the categories\n",
    "flatten_and_count(df, \"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories are fully empty so it can be dropped\n",
    "df = df.drop(\"categories\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for features column\n",
    "flatten_and_count(df, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 211,142 unique categories with most common ones as Pull On closure, Hand Wash Only, Machine Wash, Button closure, Zipper closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look into details columns\n",
    "df[\"details_keys\"] = df[\"details\"].apply(lambda details: list(details.keys()) if isinstance(details, dict) else {})\n",
    "flatten_and_count(df, \"details_keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see there 544 types of details such as model number, whether its discontinued or not. This is valuable information so let's keep it.\n",
    "- Vector database we are going to use (Weaviate) requires same schema for all dictionaries, which is not the case here.\n",
    "- So let's convert this in text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"details\"] = df[\"details\"].apply(json.dumps)\n",
    "df = df.drop(\"details_keys\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check how ratings are distributed\n",
    "\n",
    "# Histogram for average rating\n",
    "df[\"average_rating\"].dropna().hist(bins=100)\n",
    "plt.title(\"Distribution of Average Rating\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram for number of ratings\n",
    "df[\"rating_number\"].dropna().hist(bins=100)\n",
    "plt.title(\"Distribution of Rating Count\")\n",
    "plt.xlabel(\"Number of Ratings\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale(\"log\")  # optional: log scale for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check description which is suprisingly a list field (Expected a string)\n",
    "df[\"description_length\"] = df[\"description\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "df[\"description_length\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check cases where there are multiple descriptions\n",
    "df[df[\"description_length\"] > 3].head()[\"description\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these are just parts of an overall description. Let's concatenate them with space in between list elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = df[\"description\"].apply(\n",
    "    lambda x: \" \".join(x) if isinstance(x, list) else x\n",
    ")\n",
    "df = df.drop(\"description_length\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also remove parent_asin as people usually don't search using ASIN numbers\n",
    "# This is done in order to reduce the size of the dataset to below 100MB which is the limit by GitHub\n",
    "df = df.drop(\"parent_asin\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options to display full text in each column\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Display the first 20 rows of the \"main_hi_res_image\" column\n",
    "df[\"main_hi_res_image\"].dropna().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe that URL always starts with \"https://m.media-amazon.com/images/I/\"\n",
    "# (Does not necessarily end with \".jpg\" as there are .png examples)\n",
    "# Lets remove this prefix and add it later when we need to display the image\n",
    "# This will help us reduce the size of the dataset\n",
    "\n",
    "# Before that, let's validate that all URLs indeed start with this prefix\n",
    "# Define the expected format\n",
    "expected_format = r\"^https://m\\.media-amazon\\.com/images/I/.*$\"\n",
    "\n",
    "# Count rows matching the format\n",
    "matching_rows = df[\"main_hi_res_image\"].str.match(expected_format, na=False).sum()\n",
    "\n",
    "# Count rows not matching the format\n",
    "non_matching_rows = df[\"main_hi_res_image\"].notna().sum() - matching_rows\n",
    "\n",
    "matching_rows, non_matching_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, all non-null image links start with the \"https://m.media-amazon.com/images/I/\" and end with \".jpg\". Let's remove these strings from the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only the prefix from the URLs\n",
    "df[\"main_hi_res_image\"] = df[\"main_hi_res_image\"].str.replace(\n",
    "    r\"^https://m\\.media-amazon\\.com/images/I/\", \"\", regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize df and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types\n",
    "df[\"average_rating\"] = df[\"average_rating\"].astype(\"float32\")\n",
    "df[\"rating_number\"] = df[\"rating_number\"].astype(\"int32\")\n",
    "df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "df[\"store\"] = df[\"store\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage, we can save the DataFrame to a CSV file in compressed format\n",
    "# However, file size would still be above 100 MB which is the permitted size by GitHub\n",
    "# Lets remove 5% of data randomly and save the remaining data in order to reduce the size\n",
    "\n",
    "# Save the DataFrame with compression\n",
    "df_sample = df.sample(frac=0.95, random_state=42)  # Drop 5% of rows randomly\n",
    "df_sample.to_csv(\"./data/products.csv\", index=False, compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the size of saved table is below 100MB\n",
    "file_size = os.path.getsize(\"./products.csv\")\n",
    "print(f\"Size of './data/products.csv': {file_size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With removing unnecessary rows, optimizing dtypes, and dropping 5% of data, we were able to save the data with a size below 100MB\n",
    "- The resulting data file can be used to build the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
